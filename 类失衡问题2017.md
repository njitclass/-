
Unbalanced classification problems
不平衡分类问题对许多学习算法造成问题。这些问题的特点是每类问题的案件比例不均衡。
SMOTE {DMwR}
SMOTE(form, data, perc.over = 200, k = 5, perc.under = 200,
      learner = NULL, ...)

SMOTE (Chawla et. al. 2002) 是一个众所周知的打击这个问题的算法。 这种方法的总体思路是用这些案例中最近邻居人为地生成少数类的新例子。此外，多数类的示例也采用欠抽样的方式，来产生更平衡的数据集。

参数 perc.over 和 perc.under 分别控制少数类的过抽样和多数类的欠抽样。perc.over一般会是100以上的数字。 对于原始数据集中属于少数类的的每个案例，将创建该类的perc.over / 100个新示例。如果perc.over是低于100的值，则将根据原始数据集中的少数类别的随机选择的比例（由perc.over / 100给出）生成单个案例。

参数perc.under控制将被随机选择用于最终“平衡”数据集的大多数类的案例的比例。这个比例是针对新生成的少数类的案例数量来计算的。例如，如果为少数类别生成了200个新的例子，则perc.under=100（即百分数为100%）值将从原始数据集中随机选择属于多数类别的200个作为最终数据集的元素。100以上的值将从多数类别中选出更多的例子。

参数 k 控制创建新示例的方式. For each currently existing minority class example X new examples will be created (this is controlled by the parameter perc.over as mentioned above).这些例子将通过使用少数类各个例子的k个最近邻居的信息来产生。参数k控制使用这些邻居中的数量。

该函数也可用于从最终的平衡数据集中直接获取分类模型。 This can be done by including the name of the R function that implements the classifier in the parameter learner. You may also include other parameters that will be forward to this learning function. If the learner parameter is not NULL (the default) the returning value of the function will be the learned model and not the balanced data set. The function that learns the model should have as first parameter the formula describing the classification problem and in the second argument the training set.

References

Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002). Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321-357.

Torgo, L. (2010) Data Mining using R: learning with case studies, CRC Press (ISBN: 9781439810187).

http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR
> Written with [StackEdit](https://stackedit.io/).