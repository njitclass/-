Andrew Ng吴恩达在斯坦福大学的公开课
https://www.coursera.org/learn/machine-learning/
其在网易公开课的网址：
http://open.163.com/special/opencourse/machinelearning.html
Andrew Ng教授主讲的《机器学习》公开课观后感
http://blog.csdn.net/loadstar_kun/article/details/39494223

http://beader.me/mlnotebook/
台大林轩田老师开放课程-机器学习基石的讲义笔记，由学者热心整理
包括机器学习基石，机器学习技法的视频和ppt
林轩田为kdd-cup竞赛王者。

http://pan.baidu.com/s/1sly4CDf
機器學習基石(Machine Learning Foundations) 机器学习基石 课后习题链接汇总
http://blog.csdn.net/a1015553840/article/details/51085129
机器学习基石 笔记
http://blog.csdn.net/red_stone1/article/details/72870520
台大林轩田：有口碑，有故事的“机器学习”课程
http://mooc.guokr.com/post/610519/

机器学习入门者学习指南（经验分享） 
http://www.guokr.com/post/512037/
资源 | 任何阶段的学习者都适用的参考：机器学习领域书目全集
http://www.sohu.com/a/124435149_465975

https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/
Stanford online course on Statistical Learning
https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about

How IBM built Watson, its Jeopardy-playing supercomputer
Phoneme
{red soil, cotton, vegetation stubble, mixture, gray soil, damp gray soil
红土，棉花，植被茬，混合物，灰土，潮湿的灰土
seniority排行

## chapter2 统计学习
### quiz 2.1
1.2.R1
(1/1 point)
Which of the following are supervised learning problems? More than one box can be checked.

 Predict whether a website user will click on an ad   Find clusters of genes that interact with each other  Classify a handwritten digit as 0-9 from labeled examples   Find stocks that are likely to rise 
Predict whether a website user will click on an ad, Classify a handwritten digit as 0-9 from labeled examples, Find stocks that are likely to rise, - correct

EXPLANATION
Problems with clearly defined "predictors" and "responses" are supervised.

### quiz 2.2
2.1 R1
(1/1 point)
In the expression Sales ≈ f(TV, Radio, Newspaper), "Sales" is the:

 Response Response - correct  Training Data  Independent Variable  Feature
EXPLANATION

The variable which you are trying to model is called the response or outcome. The other variables are called features, predictors, or independent variables. Together, the collection of features and response values that you will use for fitting form your training data.

### quiz 2.3
2.2 R1
(1/1 point)
A hypercube with side length 1 in d dimensions is defined to be the set of points (x1, x2, ..., xd) such that 0≤xj≤1 for all j = 1, 2, ..., d. The boundary of the hypercube is defined to be the set of all points such that there exists a j for which 0≤xj≤.05 or .95≤xj≤1 (namely, the boundary is the set of all points that have at least one dimension in the most extreme 10% of possible values). What proportion of the points in a hypercube of dimension 50 are in the boundary? (hint: you may want to calculate the volume of the non-boundary region)

Please give your answer as a value between 0 and 1 with 3 significant digits. If you think the answer is 50.52%, you should say 0.505:


0.995
 correct  0.995
0.995 
EXPLANATION
We know that the volume of the whole hypercube is 1^50 = 1. The volume of the interior of the hypercube is 0.9^50 = 0.005. Thus, the volume of the boundary is 1-0.005 = 0.995.

### quiz 2.4
2.3 R1
(1 point possible)
True or False: A fitted model with more predictors will necessarily have a lower Training Set Error than a model with fewer predictors.
 
True - incorrect False

EXPLANATION
False. While we typically expect a model with more predictors to have lower Training Set Error, it is not necessarily the case. An extreme counterexample would be a case where you have a model with one predictor that is always equal to the response, compared to a model with many predictors that are random.
关键看找到的预测变量是否有解释力，没有解释力的变量再多也没用。

2.3 R2
(1/1 point)
While doing a homework assignment, you fit a Linear Model to your data set. You are thinking about changing the Linear Model to a Quadratic one. Which of the following is most likely true:

 Using the Quadratic Model will decrease your Irreducible Error.  Using the Quadratic Model will decrease the Bias of your model. Using the Quadratic Model will decrease the Bias of your model. - correct  Using the Quadratic Model will decrease the Variance of your model  Using the Quadratic Model will decrease your Reducible Error
 
EXPLANATION
Introducing the quadratic term will make your model more complicated. More complicated models typically have lower bias at the cost of higher variance. This has an unclear effect on Reducible Error (could go up or down) and no effect on Irreducible Error.

### quiz 2.5
2.4.R1
(1/1 point)
Look at the graph given on page 30 of the Chapter 2 lecture slides. Which of the following is most likely true of what would happen to the Test Error curve as we move 1/K further above 1?

 The Test Errors will increase  The Test Errors will decrease  Not enough information is given to decide  It does not make sense to have 1/K>1 It does not make sense to have 1/K>1 - correct
 
EXPLANATION
Since K is the number of neighbors, the value of K must be a Natural Number. This means that 1/K≤1.

2.R.R1
(1/1 point)
You are doing an analysis in R and need to use the 'summary()' function, but you are not exactly sure how it works. Which of the following commands should you run? (There is more than one correct answer, so any one these will earn the point).

 help(summary)   ?summary ?summary - correct  man(summary)  ?summary() 
EXPLANATION
Any of the above commands work except for 'man(summary)'. Make sure you always read the documentation so you know what functions do when you use them!
 Click to add  Bookmark
 
 ## 本章小测验
2.Q.1
(1/1 point)
For each of the following parts, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible model.

The sample size n is extremely large, and the number of predictors p is small:
 
Flexible is better - correct Flexible is better
EXPLANATION
A flexible model will allow us to take full advantage of our large sample size.

2.Q.2
(1/1 point)
The number of predictors p is extremely large, and the sample size n is small:
 
Flexible is worse - correct Flexible is worse
EXPLANATION
The flexible model will cause overfitting due to our small sample size.

2.Q.3
(1/1 point)
The relationship between the predictors and response is highly non-linear: 

Flexible is better - correct Flexible is better
EXPLANATION
A flexible model will be necessary to find the nonlinear effect.

 
2.Q.4
(1/1 point)
The variance of the error terms, i.e. σ2=Var(ϵ), is extremely high: 

Flexible is worse - correct Flexible is worse
EXPLANATION
A flexible model will cause us to fit too much of the noise in the problem.

##chapter3 线性回归
###quiz 3.1
 Click to add  Bookmark
3.1.R1
(1/1 point)
Why is linear regression important to understand? Select all that apply:

-  The linear model is often correct   
- Linear regression is very extensible and can be used to capture nonlinear effects   
- Understanding simpler methods sheds light on more complex ones 
- Linear regression is very extensible and can be used to capture nonlinear effects, Simple methods can outperform more complex ones if the data are noisy, Understanding simpler methods sheds light on more complex ones, - correct
EXPLANATION

The linear model (and every other model) is hardly ever true, but it is an important piece in many more complex methods.
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
3.1.R2
(1/1 point)
You may want to reread the paragraph on confidence intervals on page 66 of the textbook before trying this queston (the distinctions are subtle).

Which of the following are true statements? Select all that apply:

 A 95% confidence interval is a random interval that contains the true parameter 95% of the time   
 The true parameter is a random value that has 95% chance of falling in the 95% confidence interval  I perform a linear regression and get a 95% confidence interval from 0.4 to 0.5. There is a 95% probability that the true parameter is between 0.4 and 0.5.  The true parameter (unknown to me) is 0.5. If I sample data and construct a 95% confidence interval, the interval will contain 0.5 95% of the time. 
A 95% confidence interval is a random interval that contains the true parameter 95% of the time, The true parameter (unknown to me) is 0.5. If I sample data and construct a 95% confidence interval, the interval will contain 0.5 95% of the time., - correct
EXPLANATION

Confidence intervals are a "frequentist" concept: the interval, and not the true parameter, is considered random.

###quiz3.2
3.2.R1
(1/1 point)
We run a linear regression and the slope estimate is 0.5 with estimated standard error of 0.2. What is the largest value of b for which we would NOT reject the null hypothesis that β1=b? (assume normal approximation to t distribution, and that we are using the 5% significance level for a two-sided test; need two significant digits of accuracy)


0.89
 correct  0.892
0.89 
EXPLANATION

The 95% confidence interval β^1±1.96S.E.(β^1) contains all parameter values that would not be rejected at a 5% significance level.
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
3.2.R2
(1/1 point)
Which of the following indicates a fairly strong relationship between X and Y?

 R2=0.9   The p-value for the null hypothesis β1=0 is 0.0001  The t-statistic for the null hypothesis β1=0 is 30
R2=0.9, - correct
EXPLANATION

The R2 is the correlation between the two variables and measures how closely they are associated. The p value and t statistic merely measure how strong is the evidence that there is a nonzero association. Even a weak effect can be extremely significant given enough data.
###quiz 3.3
3.3.R1
(1/1 point)
Suppose we are interested in learning about a relationship between X1 and Y, which we would ideally like to interpret as causal.

True or False? 
The estimate β^1 in a linear regression that controls for many variables (that is, a regression with many predictors in addition to X1) is usually a more reliable measure of a causal relationship than β^1 from a univariate regression on X1.

 True  
 False 
answer: False - correct
EXPLANATION

Adding lots of extra predictors to the model can just as easily muddy the interpretation of β^1 as it can clarify it. One often reads in media reports of academic studies that "the investigators controlled for confounding variables," but be skeptical!
在模型中添加许多额外的预测因素可以使得系数的解释更清晰也可能更混乱。有人经常阅读媒体报道的学术研究，“调查人员控制混杂变量。。。”，但是这是值得怀疑的！
我的理解是：当加入的额外自变量与研究的自变量相关时，是无法控制额外变量不变的。
Causal inference is a difficult and slippery topic, which cannot be answered with observational data alone without additional assumptions.因果推论是一个困难和滑溜的话题，不能仅靠观察数据而无需额外的假设就回答出来。

### quiz 3.4
3.4.R1
(1/1 point)
According to the balance vs ethnicity model, what is the predicted balance for an Asian in the data set? (within 0.01 accuracy)


512.31
 correct  512.31
512.31 
EXPLANATION

For an Asian, the predicted balance is the intercept plus the Asian ethnicity effect.
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
3.4.R2
(1/1 point)
What is the predicted balance for an African American? (within .01 accuracy)


531
 correct  531
531 
EXPLANATION

For an African American, the predicted balance is just the intercept.

Note that despite the differing predictions, this difference is not statistically significant.

###quiz3.5
3.5.R1
(1/1 point)
According to the model for sales vs TV interacted with radio, what is the effect of an additional $1 of radio advertising if TV=$50? (with 4 decimal accuracy)


0.0839
 correct  .0839
0.0839 
SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
3.5.R2
(1/1 point)
What if TV=$250? (with 4 decimal accuracy)


0.3039
 correct  .3039
0.3039 
EXPLANATION

The effect of an additional unit of radio is .0289 plus .0011 times TV.

median value of owner-occupied homes in \$1000s.
自有住房的中位数为\ 1000美元。
lower status of the population (percent)
### 3.R.R1
(1/1 point)
What is the difference between lm(y ~ x*z) and lm(y ~ I(x*z)), when x and z are both numeric variables?

 The first one includes an interaction term between x and z, whereas the second uses the product of x and z as a predictor in the model.   The second one includes an interaction term between x and z, whereas the first uses the product of x and z as a predictor in the model.  The first includes only an interaction term for x and z, while the second includes both interaction effects and main effects.  The second includes only an interaction term for x and z, while the first includes both interaction effects and main effects. The second includes only an interaction term for x and z, while the first includes both interaction effects and main effects. - correct
EXPLANATION

An interaction term between a numeric x and z is just the product of x and z. The difference is that in the first model, lm processes the "*" operator between variables and automatically includes main effects, whereas in the latter model, the expression inside the I() function is not parsed as a part of the formula, but rather is simply evaluated.

### 多选题
Which of the following statements are true?

-  In the balance vs. income * student model plotted on slide 44, the estimate of beta3 is negative. 
-  One advantage of using linear models is that the true regression function is often linear.  
- If the F statistic is significant, all of the predictors have statistically significant effects.  
- In a linear regression with several variables, a variable has a positive regression coefficient if and only if its correlation with the response is positive.
In the balance vs. income * student model plotted on slide 44, the estimate of beta3 is negative., - correct
EXPLANATION

We can see that the estimate of beta3 is negative because the slope of the student line is smaller than the slope of the non-student line. That is, being a student diminishes the effect of income on balance.

The linear model is almost always wrong; however, it is often still useful.

The F statistic tests the null hypothesis that none of the predictors has any effect. Rejecting that null means concluding that *some* predictor has an effect, not that *all* of them do.

Positive correlation only means that the univariate regression has a positive correlation. See slide 20 for a counterexample.

##chapter 4 分类
###quiz 4.1
4.1 R1
(1/1 point)
Which of the following is the best example of a Qualitative Variable?

 Height  Age  Speed  Color Color - correct
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYSHOW ANSWER
4.1 R2
(1/1 point)
Judging from the plots on page 2 of the notes, which should be the better predictor of Default: Income or Balance?

 Income.  Balance. Balance. - correct  Both are equally good.  Not enough information is given to decide.
EXPLANATION

Default is clearly associated with higher balances. On the other hand, the rate of default seems fairly constant across income levels.

A simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.
The average balance that the customer has remaining on their credit card after making their monthly payment

### quiz 4.2
4.2.R1
(1/1 point)
Using the model on page 8 of the notes, what value of Balance will give a predicted Default rate of 50%? (within 3 units of accuracy)

Enter the value of Balance below:
1936.6
 correct  1936.6
1936.6 
EXPLANATION

We know that logit(.5) = β0+β1*Balance. Thus, Balance = (logit(.5) - β0)/β1 = (log(.5/(1-.5)) + 10.6513)/.0055 = 1936.6

### quiz 4.3
4.3.R1
(1/1 point)
Suppose we collect data for a group of students in a statistics class with variables X1= hours studied, X2= undergrad GPA, and Y= receive an A. We fit a logistic regression and produce estimated coefficients β^o=−6,β^1=0.05,β^2=1.

Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class (within 0.01 accuracy):


0.38
 correct  .3775
0.38 
EXPLANATION

We know that P((40,3.5))=e−6+.05∗40+1∗3.51+e−6+.05∗40+1∗3.5=.37554
SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
4.3.R2
(1/1 point)
How many hours would that student need to study to have a 50% chance of getting an A in the class?:


50
 correct  50
50 
EXPLANATION

We have P((h,3.5))=e−6+.05∗h+1∗3.51+e−6+.05∗h+1∗3.5=.5. Rearranging gives −6+.05∗h+1∗3.5=0 or h=50

### quiz 4.4
4.4 R1
(1/1 point)
In which of the following problems is Case/Control Sampling LEAST likely to make a positive impact?

 Predicting a shopper's gender based on the products they buy Predicting a shopper's gender based on the products they buy - correct Finding predictors for a certain type of cancer  Predicting if an email is Spam or Not Spam
EXPLANATION

Case/Control sampling is most effective when the prior probabilities of the classes are very unequal. We expect this to be the case for the cancer and spam problems, but not the gender problem.就是对类失衡更有效。
### 4.5 R1
(1/1 point)
Suppose that in Ad Clicks (a problem where you try to model if a user will click on a particular ad) it is well known that the majority of the time an ad is shown it will not be clicked. What is another way of saying that?

 Ad Clicks have a low Prior Probability Ad Clicks have a low Prior Probability - correct Ad Clicks have a high Prior Probability.  Ad Clicks have a low Density.  Ad Clicks have a high Density.
EXPLANATION

Whether or not an ad gets clicked is a Qualitative Variable. Thus, it does not have a density. The Prior Probability of Ad Clicks is low because most ads are not clicked.
### 4.6.R1
(1/1 point)
Which of the following is NOT a linear function in x:

 f(x)=a+b2x  The discriminant function from LDA  δk(x)=xμkσ2−μk22σ2+log⁡(πk)  logit(P(y=1|x)) where P(y=1|x) is as in logistic regression  P(y=1|x) from logistic regression P(y=1|x) from logistic regression - correct
EXPLANATION

P(y=1|x) from logistic regression is not linear because it involves both an exponential function of x and a ratio. Notice that f(x)=a+b2x is not a linear function of b, but is a linear function of x.

###4.7.R1
(1/1 point)
pg 34:

Why does Total Error keep going down on the graph on page 34 of the notes, even though the False Negative Rate increases?

 The False Negative Rate does not affect Total Error.  A higher False Negative Rate generally decreases Total Error.  Positive responses are so uncommon that the False Negatives make up only a small portion of the Total Error Positive responses are so uncommon that the False Negatives make up only a small portion of the Total Error - correct All of the above
EXPLANATION

The Total Error is a weighted average of the False Positive Rate and False Negative Rate. The weights are determined by the Prior Probabilities of Positive and Negative Responses.

### 4.8.R1
(1 point possible)
Which of the following statements best explains the relationship between Quadratic Discriminant Analysis and naive Bayes with Gaussian distributions in each class?

 Quadratic Discriminant Analysis is a more flexible class of models than naive Bayes   Quadratic Discriminant Analysis is a less flexible class of models than naive Bayes Quadratic Discriminant Analysis is a less flexible class of models than naive Bayes - incorrect Quadratic Discriminant Analysis is an equivalently flexible class of models to naive Bayes  For some problems Quadratic Discriminant Analysis is more flexible than naive Bayes, for others the opposite is true.
EXPLANATION

With Gaussian distributions, naive Bayes is equivalent to Quadratic Discriminant Analysis with the additional requirement that each class covariance matrix Σk be diagonal. Thus, Quadratic Discriminant Analysis is more flexible.
### 4.R.R1
(1/1 point)
In ch4.R, line 13 is "attach(Smarket)." If that line was omitted from the script, which of the following lines would cause an error?:

 line 15: mean(glm.pred==Direction) line 15: mean(glm.pred==Direction) - correct line 18: glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial, subset=train)  line 22: Direction.2005=Smarket$Direction[!train]  line 30: table(glm.pred,Direction.2005)
EXPLANATION

attach() allows a user to access the columns of a data.frame directly. In this case, line 15 uses "Direction" instead of "Smarket$Direction".
### 4.Q.1
(1 point possible)
Which of the following tools would be well suited for predicting if a student will get an A in a class based on the student's height, and parents’ income? Select all that apply:

 Linear Discriminant Analysis   Linear Regression   Logistic Regression   Random Guess
Linear Discriminant Analysis, Logistic Regression, - incorrect
EXPLANATION
Whether or not a student gets an A is a categorical variables. Thus, we should use a classification technique like LDA or Logistic Regression. For binary classification, linear regression and LDA are almost equivalent.

## chapter 5 重抽样方法

Well, that's because in general, the more data one, the lower the error.
If I offer you a choice of, would you rather have 100 observations or 200 observations, you'd generate like 200 observations to train on.
因为数据越多，你拥有的信息就越多，一般来说，你的误差就越低。
所以如果你的训练集只有你原来训练集的一半，它产生的错误可能要高于你想要得到的实际错误。
### 5.1.R1
(1/1 point)
When we fit a model to data, which is typically larger?

 Test Error Test Error - correct Training Error
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYSHOW ANSWER
5.1.R2
(1/1 point)
What are reasons why test error could be LESS than training error?

 By chance, the test set has easier cases than the training set.   The model is highly complex, so training error systematically overestimates test error  The model is not very complex, so training error systematically overestimates test error
By chance, the test set has easier cases than the training set., - correct
EXPLANATION

Training error usually UNDERestimates test error when the model is very complex (compared to the training set size), and is a pretty good estimate when the model is not very complex. However, it's always possible we just get too few hard-to-predict points in the test set, or too many in the training set.


Well, the thing is, we're computing the standard error if these were independent observations.
但它们并不是严格独立的。Error sub k overlaps with, Error sub j because they share some training samples. So there's some correlation between them.
But we use this anyway. We use it, and it's actually quite a good estimate.
And people have shown this mathematically.
重要的一点是，交叉验证将数据的训练部分与验证部分分开。
### 5.2.R1
(1/1 point)
Suppose we want to use cross-validation to estimate the error of the following procedure:

Step 1: Find the k variables most correlated with y

Step 2: Fit a linear regression using those variables as predictors

We will estimate the error for each k from 1 to p, and then choose the best k.

True or false: a correct cross-validation procedure will possibly choose a different set of k variables for every fold.

 TRUE TRUE - correct FALSE
EXPLANATION

True: we need to replicate our entire procedure for each training/validation split. That means the decision about which k variables are the best must be made on the basis of the training set alone. In general, different training sets will disagree on which are the best k variables.

### 5.3.R1
(1 point possible)
Suppose that we perform forward stepwise regression and use cross-validation to choose the best model size.
假设我们执行（线性）向前逐步回归，并使用交叉验证来选择最佳模型大小。

Using the full data set to choose the sequence of models is the WRONG way to do cross-validation (we need to redo the model selection step within each training fold). If we do cross-validation the WRONG way, which of the following is true?

- The selected model will probably be too complex   
- The selected model will probably be too simple 

The selected model will probably be too simple - incorrect
EXPLANATION

Using the full data set to choose the best variables means that we do not pay as much price as we should for overfitting (since we are fitting to the test and training set simultaneously). This will lead us to underestimate test error for every model size, but the bias is worst for the most complex models. Therefore, we are likely to choose a model that is more complex than the optimal model.

###5.4.R1
The use of the term bootstrap derives from the phrase to
pull oneself up by one’s bootstraps, widely thought to be
based on one of the eighteenth century \The Surprising
Adventures of Baron Munchausen" by Rudolph Erich
Raspe:
The Baron had fallen to the bottom of a deep lake. Just
when it looked like all was lost, he thought to pick himself
up by his own bootstraps.
5.4.R1
(1/1 point)
One way of carrying out the bootstrap is to average equally over all possible bootstrap samples from the original data set (where two bootstrap data sets are different if they have the same data points but in different order). Unlike the usual implementation of the bootstrap, this method has the advantage of not introducing extra noise due to resampling randomly. (You can use "^" to denote power, as in "n^2")

To carry out this implementation on a data set with n data points, how many bootstrap data sets would we need to average over?


n^n
 correct  n^n
nn 
EXPLANATION

Completely removing the bootstrap resampling noise is usually not worth incurring the extreme computational cost. If B(重抽样的次数) is large, but still less than n^n, random resampling gives a good Monte Carlo estimate of the idealized bootstrap estimate for all n^n data sets.

###5.5.R1
(1/1 point)
If we have n data points, what is the probability that a given data point does not appear in a bootstrap sample?

(1-1/n)^n
 correct  (1-1/n)^n
(1−1n)n 
EXPLANATION

To construct a bootstrap sample, we repeatedly draw a single data point from a sample of size n, n times. Any given data point has a 1-1/n chance of not being selected in each draw. Hence, the chance of not being selected in any of the n draws is (1-1/n)^n

###5.Q.1
(1/1 point)
If we use ten-fold cross-validation as a means of model selection, the cross-validation estimate of test error is:

 biased upward  
 biased downward   
any of the above potentially 

any of the above - correct
EXPLANATION

There are competing biases: on one hand, the cross-validated estimate is based on models trained on smaller training sets than the full model, which means we will tend to overestimate test error for the full model.

On the other hand, cross-validation gives a noisy estimate of test error for each candidate model, and we select the model with the best estimate. This means we are more likely to choose a model whose estimate is smaller than its true test error rate, hence, we may underestimate test error. In any given case, either source of bias may dominate the other.
 
5.Q.2
(1/1 point)
Why can't we use the standard bootstrap for some time series data?

 The data points in most time series aren't i.i.d.   
 Some points will be used twice in the same sample   
 The standard bootstrap doesn't accurately mimic the real-world data-generating mechanism 
The data points in most time series aren't i.i.d., The standard bootstrap doesn't accurately mimic the real-world data-generating mechanism, - correct
EXPLANATION

The bootstrap always involves using some points more than once in each resample, but that doesn't inherently make it incorrect (unless we are trying to gauge prediction error). The real problem in this case is that the usual bootstrap algorithm samples i.i.d., so there is no serial autocorrelation (unlike what is observed in most time series). This makes the set of resampled time series very very different from the sorts of time series we actually get in the real world. 

## chapter 6 线性模型选择和正则化
### 6.1.R1
(1/1 point)
Which of the following modeling techniques performs Feature Selection?

 Linear Discriminant Analysis  Least Squares  Linear Regression with Forward Selection Linear Regression with Forward Selection - correct Support Vector Machines
EXPLANATION

Forward Selection chooses a subset of the predictor variables for the final model. The other three methods end up using all of the predictor variables.
### 6.2.R1
(1/1 point)
We perform best subset and forward stepwise selection on a single dataset. For both approaches, we obtain p+1 models, containing 0,1,2,…,p predictors.

Which of the two models with k predictors is guaranteed to have training RSS no larger than the other model?

 Best Subset Best Subset - correct 
 Forward Stepwise   
 They always have the same training RSS   
 Not enough information is given to know

6.2.R2
(1/1 point)
Which of the two models with k predictors has the smallest test RSS?

 Best Subset   
 Forward  
 Stepwise  T 
 hey always have the same test RSS   
 Not enough information is given to know  - correct 
 
 ###6.3.R1
(1/1 point)
You are trying to fit a model and are given p=30 predictor variables to choose from. Ultimately, you want your model to be interpretable, so you decide to use Best Subset Selection.

How many different models will you end up considering?:


2^30
 correct  2^30
230 
EXPLANATION

Each predictor can either be included or not included in the model. That means that for each of the 30 variables there are two options. Thus, there are 2^30 potential models.

Note: Don’t ever try to fit that many models! It is too many and that is why Best Subset Selection is rarely used in practice for say p=10 or larger.
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
6.3.R2
(1/1 point)
How many would you fit using Forward Selection?:


466
 correct  1+30*(30+1)/2
466 
EXPLANATION

For Forward Selection, you fit (p-k) models for each k=0,...p-1. The expression for the total number of models fit is on pg 15 of the notes: p(p+1)/2+1. 

###6.4.R1
(1/1 point)
You are fitting a linear model to data assumed to have Gaussian errors. The model has up to p=5 predictors and n=100 observations. Which of the following is most likely true of the relationship between Cp and AIC in terms of using the statistic to select a number of predictors to include?

 Cp will select a model with more predictors AIC  Cp will select a model with fewer predictors AIC  Cp will select the same model as AIC Cp will select the same model as AIC - correct Not enough information is given to decide
EXPLANATION

For linear models with Gaussian errors, Cp and AIC and equivalent. 

###6.5.R1
(1/1 point)
You are doing a simulation in order to compare the effect of using Cross-Validation or a Validation set. For each iteration of the simulation, you generate new data and then use both Cross-Validation and a Validation set in order to determine the optimal number of predictors. Which of the following is most likely?

 The Cross-Validation method will result in a higher variance of optimal number of predictors  The Validation set method will result in a higher variance of optimal number of predictors The Validation set method will result in a higher variance of optimal number of predictors - correct Both methods will produce results with the same variance of optimal number of predictors  Not enough information is given to decide
EXPLANATION

Cross-Validation is similar to doing a Validation set multiple times and then averaging the answers. As such, we expect it to have lower variance than the Validation set method. This is why Cross-Validation is appealing (especially for small n). 

###6.6.R1
(1/1 point)
∑j=1pβj2 is equivalent to:

 Xβ^  β^R  Cp statistic  ∥β∥2 ∥β∥2 - correct
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYSHOW ANSWER
6.6.R2
(1/1 point)
You perform ridge regression on a problem where your third predictor, x3, is measured in dollars. You decide to refit the model after changing x3 to be measured in cents. Which of the following is true?:

 β^3 and y^ will remain the same.  β^3 will change but y^ will remain the same.  β^3 will remain the same but y^ will change.  β^3 and y^ will both change. β^3 and y^ will both change. - correct
EXPLANATION

The units of the predictors affects the L2 penalty in ridge regression, and hence β^3 and y^ will both change 

### 6.7 R1
(1/1 point)
Which of the following is NOT a benefit of the sparsity imposed by the Lasso?

 Sparse models are generally more easy to interperet  The Lasso does variable selection by default  Using the Lasso penalty helps to decrease the bias of the fits Using the Lasso penalty helps to decrease the bias of the fits - correct Using the Lasso penalty helps to decrease the variance of the fits
EXPLANATION

Restricting ourselves to simpler models by including a Lasso penalty will generally decrease the variance of the fits at the cost of higher bias. 

### 6.8 R1
(1/1 point)
Which of the following would be the worst metric to use to select λ in the Lasso?

 Cross-Validated error  Validation set error  RSS RSS - correct
EXPLANATION

RSS would be the worst metric to use because it will cause us to always select the most complicated model. Any of the other metrics could be used, although Cross-Validated error is probably most common. 
 
### 6.9.R1
(1/1 point)
We compute the principal components of our p predictor variables. The RSS in a simple linear regression of Y onto the largest principal component will always be no larger than the RSS in a simple regression of Y onto the second largest principal component. True or False? (You may want to watch 6.10 as well before answering - sorry!)

 True  False False - correct
EXPLANATION

Principal components are found independently of Y, so we can't know the relationship with Y a priori. 

### 6.10.R1
(1/1 point)
You are working on a regression problem with many variables, so you decide to do Principal Components Analysis first and then fit the regression to the first 2 principal components. Which of the following would you expect to happen?:

 A subset of the features will be selected  Model Bias will decrease relative to the full least squares model  Variance of fitted values will decrease relative to the full least squares model Variance of fitted values will decrease relative to the full least squares model - correct Model interpretability will improve relative to the full least squares model
EXPLANATION

While some forms of dimensional reduction will cause the first or fourth to occur, that is not the case with PCA. When using dimensional reduction we restrict ourselves to simpler models. Thus, we expect bias to increase and variance to decrease. 

###6.R.R1
(1/1 point)
One of the functions in the glmnet package is cv.glmnet(). This function, like many functions in R, will return a list object that contains various outputs of interest. What is the name of the component that contains a vector of the mean cross-validated errors?


cvm
 cvm - correct  cvm
EXPLANATION

To check the return value of a function, use ?functionname and then scroll down to the Value heading. 
###6.Q.1
(1/1 point)
Suppose we estimate the regression coefficients in a linear regression model by minimizing

∑i=1n(yi−β0−∑j=1pβjxij)2+λ∑j=1pβj2
for a particular value of λ. For each of the following, select the correct answer:

As we increase λ from 0, the training RSS will:
Steadily increase - correct Steadily increase
EXPLANATION

Increasing λ will force us to fit simpler models. This means that training RSS will steadily increase because we are less able to fit the training data exactly.

6.Q.2
(1/1 point)
As we increase λ from 0, the test RSS will: 
Decrease initially, and then eventually start increasing in a U shape - correct Decrease initially, and then eventually start increasing in a U shape
At first, we expect test RSS to improve because we are not overfitting our training data anymore. Eventually, we will start fitting models that are too simple to capture the true effects and test RSS will go up.

6.Q.3
(1/1 point)
As we increase λ from 0, the variance will: 
Steadily decrease - correct Steadily decrease
Increasing λ will cause us to fit simpler models, which reduces the variance of the fits.

6.Q.4
(1/1 point)
As we increase λ from 0, the (squared) bias will: 
Steadily increase - correct Steadily increase
Increasing λ will cause us to fit simpler models, which have larger squared bias.

6.Q.5
(1/1 point)
As we increase λ from 0, the irreducible error will: 
Remain constant - correct Remain constant
Increasing λ will have no effect on irreducible error. By definition, irreducible error is an aspect of the problem and has nothing to do with a particular model being fit.


## chapter 7 超越线性
### 7.1.R1
(1/1 point)
Which of the following can we add to linear models to capture

nonlinear effects?

 Spline terms   Polynomial terms   Interactions   Arbitrary linear combinations of the variables  Step functions 
Spline terms, Polynomial terms, Interactions, Step functions, - correct
EXPLANATION

If we add any of these terms to our linear model, the

model will be able to capture new nonlinear effects. The only exception is linear combinations of variables already in the model: any linear combination of those variables is already included in the model.

### 

### 

### 

### 


## chapter 8 树类方法
###8.1.R1
(1/1 point)
Using the decision tree on page 5 of the notes, what would you predict for the log salary of a player who has played for 4 years and has 150 hits?:

 5.11 5.11 - correct 5.55  6.0  6.74
EXPLANATION

The player has played less than 4.5 years, so at the first split we follow the left branch. There are no further splits, so we predict 5.11.
###8.2.R1
(1/1 point)
Imagine that you are doing cost complexity pruning as defined on page 18 of the notes. You fit two trees to the same data: T1 is fit at α=1 and T2 is fit at α=2. Which of the following is true?

 T1 will have at least as many nodes as T2 T1 will have at least as many nodes as T2 - correct T1 will have at most as many nodes as T2  Not enough information is given in the problem to decide
EXPLANATION

A higher value of alpha corresponds to a higher penalty on the complexity of the tree. This means that T1 will have at least as many nodes as T2.
### 8.3.R1
(1 point possible)
You have a bag of marbles with 64 red marbles and 36 blue marbles.

What is the value of the Gini Index for that bag? Give your answer to the nearest hundredth:
0.23
 incorrect  .461
0.23 
EXPLANATION

Using the formula from pgs 25-26:
Gini Index = .64*(1-.64) + .36*(1-.36) = .4608
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
8.3.R2
(1/1 point)
What is the value of the Cross-Entropy? Give your answer to the nearest hundredth (using log base e, as in R):
0.65
 correct  .653
0.65 
EXPLANATION
Cross Entropy = -.64*ln(.64) - .36*ln(.36) = .6534

### 8.4.R1
(1/1 point)
Suppose we produce ten bootstrap samples from a data set containing red and green classes. We then apply a classification tree to each bootstrap sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X):

0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7, and 0.75
There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in the notes. The second approach is to classify based on the average probability.

What is the final classification under the majority vote method?: 
red - correct red
EXPLANATION

6 of the 10 probabilities are greater than 1/2, so the majority vote method will select red.

8.4.R2
(1/1 point)
What is the final classification under the average probability method?: 
green - correct green
EXPLANATION

The average of the probabilities is 0.45, so the average probability method will select green.

###8.5.R1
(1/1 point)
In order to perform Boosting, we need to select 3 parameters: number of samples B, tree depth d, and step size λ.

How many parameters do we need to select in order to perform Random Forests?:


2
 correct  2
2 
EXPLANATION

To perform Random Forests we need to select 2 parameters: number of samples B and m= number of variables sampled at each split.

### 8.R.R1
(1/1 point)
You are trying to reproduce the results of the R labs, so you run the following command in R:

> library(tree)

As a response, you see the following error message:

Error in library(tree) : there is no package called ‘tree’

What went wrong?

 You meant to use 'require(tree)'  You meant to use 'library("tree")'  The tree package is not installed on your computer The tree package is not installed on your computer - correct Nothing is wrong, that error message could not be produced by R

###8.Q1
(1/1 point)
The tree building algorithm given on pg 13 is described as a Greedy Algorithm. Which of the following is also an example of a Greedy Algorithm?:

 The Lasso  Support Vector Machines  The Bootstrap  Forward Stepwise Selection Forward Stepwise Selection - correct
EXPLANATION
Forward Stepwise Selection is a Greedy Algorithm because at each step it selects the variable that improves the current model the most. There is no guarantee that the final result will be optimal.前进逐步选择是一种贪婪算法，因为在每个步骤中，它选择最能改进当前模型的变量。不能保证最终结果是最佳的。贪婪似乎指一步最优，不是全局最优。

8.Q2
(1/1 point)
Examine the plot on pg 23. Assume that we wanted to select a model using the one-standard-error rule on the Cross-Validated error. What tree size would end up being selected?:

 1  2 2 - correct 3  10
EXPLANATION
Cross-Validated error is minimized at tree size 3. The error for Tree size 2 is within one standard error of the minimum, but the error for tree size 1 is not. Thus, the one-standard-error rule selects tree size 2.交叉验证的误差在树形大小为3时最小。树大小为2的误差在最小值的一个标准误差范围内，但树形大小误差1的误差则不在此范围内。因此，单一标准误差规则选择树形大小为2。

8.Q3
(1/1 point)
Suppose I have two qualitative predictor variables, each with three levels, and a quantitative response. I am considering fitting either a tree or an additive model. For the additive model, I will use a piecewise-constant function for each variable, with a separate constant for each level. Which model is capable of fitting a richer class of functions:

 Tree Tree - correct Additive Model  They are equivalent
EXPLANATION
Although each split of the tree makes two partitions, it can fit interactions, while the additive model cannot. With enough splits the tree can fit a full 3x3 regression surface.虽然树的每个分割都有两个分区，但它可以适应相互作用，而可加模型则不能。有了足够的分裂点，树可以适应一个完整的3x3回归表面。

## chapter 9 支持向量机

## chapter10 无监督学习
### 10.1.R1
(1/1 point)
You are analyzing a dataset where each observation is an age, height, length, and width of a particular turtle. You want to know if the data can be well described by fewer than four dimensions (maybe for plotting), so you decide to do Principal Component Analysis. Which of the following is most likely to be the loadings of the first Principal Component?

 (1, 1, 1, 1)  
 (.5, .5, .5, .5) (.5, .5, .5, .5) - correct  
 (.71, -.71, 0, 0)  
 (1, -1, -1, -1)
 
EXPLANATION

We know that options 1 and 4 cannot be right because the sum of the squared loadings exceeds 1. The second option is most likely correct because we expect all four variables to be positively correlated with each-other.

Note that it is fairly common for the loadings of the first principal component to all have the same sign. In such a case, the principal component is often referred to as a size component.

###10.2.R1
(1/1 point)
Suppose we a data set where each data point represents a single student's scores on a math test, a physics test, a reading comprehension test, and a vocabulary test.

We find the first two principal components, which capture 90% of the variability in the data, and interpret their loadings. We conclude that the first principal component represents overall academic ability, and the second represents a contrast between quantitative ability and verbal ability.

What loadings would be consistent with that interpretation? Choose all that apply.

 (0.5, 0.5, 0.5, 0.5) and (0.71, 0.71, 0, 0)   
 (0.5, 0.5, 0.5, 0.5) and (0, 0, -0.71, -0.71)   
 (0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5)    
 (0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5)    
 (0.71, 0.71, 0, 0) and (0, 0, 0.71, 0.71)   
 (0.71, 0, -0.71, 0) and (0, 0.71, 0, -0.71) 
 
(0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5), (0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5), - correct
EXPLANATION

For the first two choices, the two loading vectors are not orthogonal. For the fifth and sixth choices, the first set of loadings only has to do with two specific tests. For the third and fourth pairs of loadings, the first component is proportional to average score, and the second component measures the difference between the first pair of scores and the second pair of scores. 

### 10.3.R1
(1/1 point)
True or False: If we use k-means clustering, will we get the same cluster assignments for each point, whether or not we standardize the variables.

  True  
  False  

False - correct

EXPLANATION

The points are assigned to centroids using Euclidean distance. If we change the scaling of one variable, e.g. by dividing it by 10, then that variable will matter less in determining Euclidean distance.
 
### 10.4.R1
(1/1 point)
True or False: If we cut the dendrogram at a lower point, we will tend to get more clusters (and cannot get fewer clusters).

 True  
 True - correct False
EXPLANATION

After cutting the dendrogram at threshold t, we keep all the joins with linkage distance less than t and discard the joins with larger linkage distance. Thus, decreasing the threshold gives us fewer joins, and thus more clusters. If, in decreasing the threshold, we don't cross a junction of the dendrogram, the number of clusters will remain the same.

### 10.5.R1
(1/1 point)
In the heat map for breast cancer data, which of the following depended on the output of hierarchical clustering?

 The ordering of the rows    
 The ordering of the columns    
 The coloring of the cells as red or green
The ordering of the rows, The ordering of the columns, - correct
EXPLANATION

The dendrograms obtained from hierarchical clustering were used to order the rows and columns. The coloring of the cells was based on gene expression, but without the hierarchical clustering step, the heat map would not have looked like anything meaningful.

### 10.R.1
(1/1 point)
Suppose we want to fit a linear regression, but the number of variables is much larger than the number of observations. In some cases, we may improve the fit by reducing the dimension of the features before.

In this problem, we use a data set with n = 300 and p = 200, so we have more observations than variables, but not by much. Load the data x, y, x.test, and y.test from 10.R.RData.

First, concatenate x and x.test using the rbind functions and perform a principal components analysis on the concatenated data frame (use the "scale=TRUE" option). To within 10% relative error, what proportion of the variance is explained by the first five principal components?


0.3499
 correct  0.34986
0.3499 
EXPLANATION

use "vars = prcomp(rbind(x,x.test),scale=TRUE)$sdev^2"
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
10.R.2
(1/1 point)
The previous answer suggests that a relatively small number of "latent variables" account for a substantial fraction of the features' variability. We might believe that these latent variables are more important than linear combinations of the features that have low variance.

We can try forgetting about the raw features and using the first five principal components (computed on rbind(x,x.test)) instead as low-dimensional derived features. What is the mean-squared test error if we regress y on the first five principal components, and use the resulting model to predict y.test?


0.99
 correct  0.9923
0.99 
EXPLANATION

In the actual data generating model for this example, the features may be noisy proxies for a few latent variables that actually drive the response. This is not an uncommon situation when we have high-dimensional data.
 SUBMIT YOUR ANSWER  TOGGLE ANSWER VISIBILITYHIDE ANSWER
10.R.3
(1/1 point)
Now, try an OLS linear regression of y on the matrix x. What is the mean squared predition error if we use the fitted model to predict y.test from x.test?


3.6572
 correct  3.657
3.6572 
EXPLANATION

The mean squared error is worse because of the variance involved in fitting a very high dimensional model. As it turned out here, the large-variance directions of x turned out to be the important ones for predicting y. Note that this need not always be the case, but it often is.
Violent Crime Rates by US State

Description

This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.

###10.Q.1 key
(1/1 point)
K-Means is a seemingly complicated clustering algorithms. Here is a simpler one:

Given k, the number of clusters, and n, the number of observations, try all possible assignments of the n observations into k clusters. Then, select one of the assignments that minimizes Within-Cluster Variation as defined on page 30.

Assume that you implemented the most naive version of the above algorithm. Here, by naive we mean that you try all possible assignments even though some of them might be redundant (for example, the algorithm tries assigning all of the observations to cluster 1 and it also tries to assign them all to cluster 2 even though those are effectively the same solution).

In terms of n and k, how many potential solutions will your algorithm try?



k^n
 correct  k^n
kn 
EXPLANATION

For each of the n observations we have k options for assignment. Each of the assignments is done independently, so k^n.

Note, the exponential explosion in the number of potential solutions is the reason we need to use greedy algorithms like K-Means in order to perform clustering.




> Written with [StackEdit](https://stackedit.io/).
> the estimates of prediction error will typically be biased upward
Plot the columns of one matrix against the columns of another.