GLM Residuals and Diagnostics

- 在模型拟合之后，明智的做法是检查模型以了解其与数据的匹配度。
- 在线性回归中，这些诊断是围绕残差和残差平方和建立的。
- 在逻辑回归（和所有广义线性模型）中，是几种不同种类的残差（因此，也有与残差平方和对应不同等价物）

## 卡方检验
需要注意 SAS和 R中会默认报告整个模型的卡方检验结果。
这是模型与仅有常数项（null）模型相比的似然比测试，类似于线性回归中的“总体F检验”。
该测试有时用于证明模型的正确性，然而这是错误的。

就像所有基于模型的推理一样，似然比检验在模型成立的假设下是合理的。
因此，F检验将模型视为给定的，并且不可能是模型有效性的检验。
卡方检验能够得到的唯一结论是,若模型是正确的，它的系数至少有一个非零 （这有用吗？）
解决模型的有效性和稳定性比简单的测试要复杂得多。

## Pearson residuals
该残差将原始数值减去均值后除以标准差。
对应logistic回归,
$$r_i =\frac{y_i −\hat\pi _i}{\sqrt{\hat\pi_i(1-\hat\pi_i)}}$$
注：若将$\hat\pi_i$替换为$\pi_i$,则该残差的均值为0方差为1。


## Deviance residuals
该残差基于各数据点对似然之贡献。
对应logistic回归,
$lnL=\sum_i {y_i log\hat\pi_i+(1-y_i)}log(1-\hat\pi_i)$
类似于线性回归，定义离差残差 ：
$$d_i = s_i\sqrt{−2 [y_i log \hat\pi_i + (1 − y_i) log(1 − \hat\pi_i)]}$$
其中$s_i = 1$ if $y_i = 1$ and $s_i = −1$ if $y_i = 0$

##Deviance和 Pearson统计量
这些类型的残差中的每一个都可以进行平方求和以产生类似RSS的统计量。
离差残差的平方和得到离差:
$D = \sum_id_i^2=-2lnL$

Pearson残差的平方和得到 Pearson统计量:
$X^2 = \sum_ir_i^2$

##Goodness of fit tests拟合优度检验
原则上，两个统计量可以与自由度n-p的卡方分布进行比较，作为粗糙的拟合度检验。
However, this test does not actually work very well
Several modifications have been proposed, including an early
test proposed by Hosmer and Lemeshow that remains popular
and is available in SAS
Other, better tests have been proposed as well (an extensive
comparison was made by Hosmer et al. (1997))

##  GLM的帽子矩阵
在线性回归中in linear regression it was important to
divide by p1 − Hii to account for the leverage that a point
had over its own fit
Similar steps can be taken for logistic regression; here, the
投影矩阵（帽子矩阵）
$H = W^\frac{1}{2}X(X^T WX)^{−1}X^TW^\frac{1}{2}$;
其中，$W^\frac{1}{2}$是对角阵$ W_{ii}^\frac{1}{2}= \sqrt{w_i}$

## 帽子矩阵的特性
在 logistic回归中, $\hat \pi \ne Hy ${ no matrix can satisfy this
requirement, as logistic regression does not produce linear
estimates
但是,它有一些线性回归投影矩阵的特性:
$Hr = 0$
H 是对称的
H 是幂等
$H  W^\frac{1}{2}X=W^\frac{1}{2}X$和$ X^TW^\frac{1}{2} H = X^T W^\frac{1}{2}$
其中r 为Pearson残差向量

##标准化残差
H的对角元素又被称为杠杆，并用于标准化残差:
$r_{si }=\frac{r_i}{\sqrt{1-H_{ii}}}$
$d_{si }=\frac{d_i}{\sqrt{1-H_{ii}}}$
一般来说，标准化离差残差往往是更受偏爱，因为它们比标准化的Pearson残差更对称，但两者都是常用的。

##Leave-one-out diagnostics
您可能还记得，在线性回归方面，有一些诊断措施是基于剔除观察记录i，重新拟合模型，看会有怎样的变化(residuals, coefficient estimates, fitted values)
您也可以记得，对于线性回归，实际上并不需要重新拟合模型n次，可以使用基于H的显式表达式的快捷方式。
同样的想法可以扩展到广义线性模型，尽管我们在不进行近似的条件下是不能利用显式解决方案的。

## One-step approximations
所得到的近似统计量被称为是对真值的一步近似。
问题在于，我们可以快速计算基于当前权重{wi}的一步近似，而不需要重新设计任何东西，但为了计算确切的值，我们需要通过n个IRLS算法.
近似值通常相当不错，但是如果一个点的影响非常大，则近似值可能与真实值有很大的不同。

## 
一步近似使我们能够快速计算以下GLM诊断统计量：

-  学生化删除残差
- $\Delta\beta $(用于评估个别系数的变化)
- Cook距离 (用于评估对模型拟合的整体影响)

## 方差膨胀因子Variance inflation factors
It is worth mentioning variance inflation factors (VIF) briefly
here
VIF是自变量 X 的一个函数, 因此VIF的计算以及其含义与线性回归在本质上是定价的 (\essentially equivalent" because we do
have weights for GLMs)
在 R中, 我们可以用car包的 vif函数来计算:
> vif(fit)
Age |Sex |Age:Sex
--|---|--
7.416253| 14.159377| 16.989516


## 多重共线性 Multicollinearity
If you believe multicollinearity to be a problem, it is often a
good idea to look at the correlation matrix for X:
```r
cor(model.matrix(fit)[,-1])
```
Age Sex Age:Sex
Age 1.00 0.04 0.52
Sex 0.04 1.00 0.82
Age:Sex 0.52 0.82 1.00
在这个模型中，我们肯定会引入很多变异性，包括交互;
另一方面，互动项确实似乎很重要 p = 0.05

## Leverage

为了了解这些统计数据所传达的信息，我们来看看Donner派对数据的各种曲线，从杠杆开始:
Age
Leverage
0.00
0.05
0.10
0.15
0.20
20 30 40 50 60
Female
20 30 40 50 6

## Cook’s Distance

##Delta-beta (for effect of age)

##Residuals / proportional leverage

##Summary
逻辑回归的残差肯定比线性回归具有更少的信息:
不仅是/否结果固有地包含比连续变量的信息更少的信息，而且调整后的响应取决于拟合值的事实也阻碍了我们使用残差作为模型的外部检查的能力。

- 然而，这在某种程度上得到缓解，因为我们在逻辑回归中也减少了分布假设，所以没有必要检查残差的偏度或异方差。
- 在logistic回归中异常值和有影响力的观察结果，与线性回归相似
- 在我看来，检查库克距离的情况几乎永远不会浪费时间。
- 如果存在有影响力的观察记录，则改变模型可能或可能不合适，但您应该至少了解为什么一些观察记录如此有影响力

## 方差膨胀Variance inflation

最后，请记住，虽然多重共线性和方差膨胀是重要的概念，但并不总是需要计算VIF来评估它们。
建模时从简单的模型开始，并逐渐增加复杂度，通常是一个好主意。
如果添加一个变量或相互作用，标准错误显著增加，这是直接的需要观察VIF的重要信号！
> Written with [StackEdit](https://stackedit.io/).